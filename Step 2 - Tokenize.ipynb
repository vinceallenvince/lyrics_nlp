{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "import itertools\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and create a Series\n",
    "Load a compressed dataframe and converts to a Series after a first pass at cleaning up the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialClean(text):\n",
    "    # remove newlines, doubled singlequotes, excess whitespace\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\'\\'\", \"\").strip()\n",
    "    # remove text enclosed in brackets; ex: [Verse 1]\n",
    "    text = re.sub(r'\\[.+?\\]', '', text).strip()\n",
    "    # remove text beginning with a single quote; ex: '01, 'em\n",
    "    text = re.sub(r\"\\'\\w+\", \"\", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>036B7lKiRkLerLGI6EHtEr.txt</td>\n",
       "      <td>[Thanks A Lot]\\n[Verse]\\nYou're telling everyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Avmi9t3sOcaGSs1DSbgDg.txt</td>\n",
       "      <td>[Folsom Prison Blues]\\nI hear the train a comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FR4Ua3VxoSVA7DOFtdPlO.txt</td>\n",
       "      <td>[Drink To Me]\\n[Chorus]\\nDrink to me, drink to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0KSHmjK7OFtGocvbo7NZNO.txt</td>\n",
       "      <td>[Five Feet High And Rising]\\n[Chorus]\\nHow hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0PlyzrcKNoaTo5lAVzZCKE.txt</td>\n",
       "      <td>[Ballad Of A Teenage Queen]\\n(Dream on, dream ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          uri  \\\n",
       "0  036B7lKiRkLerLGI6EHtEr.txt   \n",
       "1  0Avmi9t3sOcaGSs1DSbgDg.txt   \n",
       "2  0FR4Ua3VxoSVA7DOFtdPlO.txt   \n",
       "3  0KSHmjK7OFtGocvbo7NZNO.txt   \n",
       "4  0PlyzrcKNoaTo5lAVzZCKE.txt   \n",
       "\n",
       "                                              lyrics  \n",
       "0  [Thanks A Lot]\\n[Verse]\\nYou're telling everyo...  \n",
       "1  [Folsom Prison Blues]\\nI hear the train a comi...  \n",
       "2  [Drink To Me]\\n[Chorus]\\nDrink to me, drink to...  \n",
       "3  [Five Feet High And Rising]\\n[Chorus]\\nHow hig...  \n",
       "4  [Ballad Of A Teenage Queen]\\n(Dream on, dream ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the raw lyrics from a Pandas DataFrame saved as a csv.\n",
    "lyrics_df = pd.read_csv(\"./corpus/lyrics/lyrics_raw.gz\", compression='gzip', names=['uri', 'lyrics'])\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyrics_raw = []\n",
    "for lyrics in lyrics_df['lyrics']:\n",
    "    lyrics_raw.append(initialClean(lyrics))\n",
    "\n",
    "# create a Series to index lyrics by filename\n",
    "lyrics_by_file = Series(lyrics_raw,index=lyrics_df['uri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "We'll tokenize using a regular expression tokenizer to preserve contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "regexpTokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uri\n",
       "036B7lKiRkLerLGI6EHtEr.txt    You telling everyone in town that I don treat ...\n",
       "0Avmi9t3sOcaGSs1DSbgDg.txt    I hear the train a comin' It rolling the bend ...\n",
       "0FR4Ua3VxoSVA7DOFtdPlO.txt    Drink to me drink to me Drink to me drink to m...\n",
       "0KSHmjK7OFtGocvbo7NZNO.txt    How high the water mama Two feet high and risi...\n",
       "0PlyzrcKNoaTo5lAVzZCKE.txt    Dream on dream on teenage queen prettiest girl...\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for filen, lyrics in lyrics_by_file.iteritems():\n",
    "    lyrics_by_file[filen] = \" \".join(regexpTokenizer.tokenize(lyrics))\n",
    "lyrics_by_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah', 'yea', 'ooh', 'ahh', 'ahhh', 'ahhhh', 'ahhhhh', 'huh', 'hey', 'hmm', 'mmm', 'mhm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')\n",
    "custom_stops = []\n",
    "with open(\"./resources/custom_stopwords.txt\", 'rb') as handle:\n",
    "    custom_stops = handle.read().split(\"\\n\")\n",
    "    print custom_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings \"\"\"\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation) and \n",
    "                   (token.lower() not in custom_stops) and (token.lower() not in english_stops) and len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinceallen/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uri\n",
       "036B7lKiRkLerLGI6EHtEr.txt    telling everyone town treat right even say sta...\n",
       "0Avmi9t3sOcaGSs1DSbgDg.txt    hear train comin' rolling bend seen sunshine s...\n",
       "0FR4Ua3VxoSVA7DOFtdPlO.txt    drink drink drink drink drink rose carnation l...\n",
       "0KSHmjK7OFtGocvbo7NZNO.txt    high water mama two feet high rising high wate...\n",
       "0PlyzrcKNoaTo5lAVzZCKE.txt    dream dream teenage queen prettiest girl ever ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_clean = []\n",
    "for filen, lyrics in lyrics_by_file.iteritems():\n",
    "    lyrics_clean.append(\" \".join(clean_tokens(lyrics.split(' '))))\n",
    "    \n",
    "# create a Series to index cleaned lyrics by filename\n",
    "lyrics_clean_by_file = Series(lyrics_clean,index=lyrics_by_file.index)\n",
    "lyrics_clean_by_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total raw tokens: 10718\n",
      "total clean tokens: 5056\n",
      "53% reduction\n"
     ]
    }
   ],
   "source": [
    "total_raw_tokens = 0\n",
    "for filen, lyrics in lyrics_by_file.iteritems():\n",
    "    total_raw_tokens += len(lyrics.split(' '))\n",
    "\n",
    "total_clean_tokens = 0\n",
    "lyrics_clean = \"\"\n",
    "for filen, lyrics in lyrics_clean_by_file.iteritems():\n",
    "    total_clean_tokens += len(lyrics.split(' '))\n",
    "    lyrics_clean = lyrics_clean + lyrics\n",
    "    \n",
    "print \"total raw tokens: {0}\".format(total_raw_tokens)\n",
    "print \"total clean tokens: {0}\".format(total_clean_tokens)\n",
    "print \"{0:.0f}% reduction\".format(100 - total_clean_tokens / total_raw_tokens * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.73 ms, sys: 815 Âµs, total: 3.54 ms\n",
      "Wall time: 3.69 ms\n"
     ]
    }
   ],
   "source": [
    "# combine all cleaned tokens in an list an sort\n",
    "%time sorted_clean = sorted(lyrics_clean.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abused',\n",
       " 'accidentally',\n",
       " 'aches',\n",
       " 'achesyoung',\n",
       " 'aching',\n",
       " 'aching',\n",
       " 'aching',\n",
       " 'acres',\n",
       " 'acting',\n",
       " 'admit']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample clean tokens to see if we still have garbage\n",
    "sorted_clean[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tokens\n",
    "We'll use this file in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeFile(path, text):\n",
    "    text_file = open(path, \"w\") \n",
    "    text_file.write(text + \" \")\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filen, lyrics in lyrics_clean_by_file.iteritems():\n",
    "    writeFile(\"./corpus/lyrics/tokenized/\" + filen.replace(\"spotify:track:\", \"\") + \".txt\", lyrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
